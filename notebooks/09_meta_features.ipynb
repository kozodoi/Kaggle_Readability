{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "realistic-emerald",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:36:01.879745Z",
     "iopub.status.busy": "2021-07-20T10:36:01.878476Z",
     "iopub.status.idle": "2021-07-20T10:36:59.400251Z",
     "shell.execute_reply": "2021-07-20T10:36:59.399207Z",
     "shell.execute_reply.started": "2021-07-18T07:33:59.881663Z"
    },
    "papermill": {
     "duration": 57.554364,
     "end_time": "2021-07-20T10:36:59.400471",
     "exception": false,
     "start_time": "2021-07-20T10:36:01.846107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/textstat/Pyphen-0.10.0-py3-none-any.whl\r\n",
      "Installing collected packages: Pyphen\r\n",
      "Successfully installed Pyphen-0.10.0\r\n",
      "Processing /kaggle/input/textstat/textstat-0.7.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyphen in /opt/conda/lib/python3.7/site-packages (from textstat==0.7.0) (0.10.0)\r\n",
      "Installing collected packages: textstat\r\n",
      "Successfully installed textstat-0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "####### PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "!pip install '../input/textstat/Pyphen-0.10.0-py3-none-any.whl'\n",
    "!pip install '../input/textstat/textstat-0.7.0-py3-none-any.whl'\n",
    "sys.path = ['../input/readability-package'] + sys.path\n",
    "\n",
    "import readability\n",
    "import spacy\n",
    "from textstat import textstat\n",
    "\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contemporary-workplace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:36:59.458557Z",
     "iopub.status.busy": "2021-07-20T10:36:59.431760Z",
     "iopub.status.idle": "2021-07-20T10:37:00.744345Z",
     "shell.execute_reply": "2021-07-20T10:37:00.743681Z",
     "shell.execute_reply.started": "2021-07-18T07:37:37.422944Z"
    },
    "papermill": {
     "duration": 1.333105,
     "end_time": "2021-07-20T10:37:00.744513",
     "exception": false,
     "start_time": "2021-07-20T10:36:59.411408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### HELPER FUNCTIONS\n",
    "\n",
    "nlp         = en_core_web_sm.load()\n",
    "STOPWORDS   = stopwords.words(\"english\")\n",
    "PUNCTUATION = list(string.punctuation)\n",
    "POS_TAGS    = [\"ADJ\",\"ADP\",\"ADV\",\"AUX\",\"CCONJ\",\"DET\",\"INTJ\",\"NOUN\",\"NUM\",\"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"VERB\",\"X\",\"SPACE\"]\n",
    "\n",
    "\n",
    "def readability_measurements(passage: str):\n",
    "    \"\"\"\n",
    "    This function uses the readability library for feature engineering.\n",
    "    It includes textual statistics, readability scales and metric, and some pos stats\n",
    "    \"\"\"\n",
    "    results = readability.getmeasures(passage, lang='en')\n",
    "    \n",
    "    complex_words  = results['sentence info']['complex_words']\n",
    "    long_words     = results['sentence info']['long_words']\n",
    "    \n",
    "    kincaid      = results['readability grades']['Kincaid']\n",
    "    ari          = results['readability grades']['ARI']\n",
    "    coleman_liau = results['readability grades']['Coleman-Liau']\n",
    "    flesch       = results['readability grades']['FleschReadingEase']\n",
    "    gunning_fog  = results['readability grades']['GunningFogIndex']\n",
    "    lix          = results['readability grades']['LIX']\n",
    "    smog         = results['readability grades']['SMOGIndex']\n",
    "    rix          = results['readability grades']['RIX']\n",
    "    dale_chall   = results['readability grades']['DaleChallIndex']\n",
    "    \n",
    "    tobeverb       = results['word usage']['tobeverb']\n",
    "    auxverb        = results['word usage']['auxverb']\n",
    "    conjunction    = results['word usage']['conjunction']\n",
    "    pronoun        = results['word usage']['pronoun']\n",
    "    preposition    = results['word usage']['preposition']\n",
    "    nominalization = results['word usage']['nominalization']\n",
    "    \n",
    "    pronoun_b     = results['sentence beginnings']['pronoun']\n",
    "    interrogative = results['sentence beginnings']['interrogative']\n",
    "    article       = results['sentence beginnings']['article']\n",
    "    subordination = results['sentence beginnings']['subordination']\n",
    "    conjunction_b = results['sentence beginnings']['conjunction']\n",
    "    preposition_b = results['sentence beginnings']['preposition']\n",
    "\n",
    "    \n",
    "    return [complex_words, long_words,\n",
    "            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n",
    "            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n",
    "            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simplify_punctuation(text):\n",
    "    # from https://github.com/shivam5992/textstat/issues/77\n",
    "    text = re.sub(r\"[,:;()\\-]\", \" \", text)  # Override commas, colons, etc to spaces/\n",
    "    text = re.sub(r\"[\\.!?]\", \".\", text)  # Change all terminators like ! and ? to \".\"\n",
    "    text = re.sub(r\"^\\s+\", \"\", text)  # Remove white space\n",
    "    text = re.sub(r\"[ ]*(\\n|\\r\\n|\\r)[ ]*\", \" \", text)  # Remove new lines\n",
    "    text = re.sub(r\"([\\.])[\\. ]+\", \".\", text)  # Change all \"..\" to \".\"\n",
    "    text = re.sub(r\"[ ]*([\\.])\", \". \", text)  # Normalize all \".\"`\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+$\", \"\", text)  # Remove trailing spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_mean_parse_tree_depth(text):\n",
    "    sentences = text.split(\".\")\n",
    "    depths = []\n",
    "    for doc in list(nlp.pipe(sentences)):\n",
    "        depths += get_parse_tree_depths(doc)\n",
    "    return np.mean(depths)\n",
    "\n",
    "\n",
    "def get_parse_tree_depths(doc):\n",
    "    return [get_depth(token) for token in doc]\n",
    "\n",
    "\n",
    "def get_depth(token, depth=0):\n",
    "    depths = [get_depth(child, depth + 1) for child in token.children]\n",
    "    return max(depths) if len(depths) > 0 else depth\n",
    "\n",
    "\n",
    "def get_mean_pos_tags(text):\n",
    "    sentences       = text.split(\".\")\n",
    "    sentence_counts = make_pos_tag_count_lists(sentences)\n",
    "    num_sentences   = textstat.sentence_count(text)\n",
    "    mean_pos_tags   = calculate_mean_per_tag(sentence_counts, num_sentences)\n",
    "    return mean_pos_tags\n",
    "\n",
    "\n",
    "def make_pos_tag_count_lists(sentences):\n",
    "    sentence_counts = {}\n",
    "    for doc in list(nlp.pipe(sentences)):\n",
    "        pos_counts = get_pos_tag_counts(doc)\n",
    "        for key in pos_counts:\n",
    "            if key in sentence_counts:\n",
    "                sentence_counts[key].append(pos_counts[key])\n",
    "            else:\n",
    "                sentence_counts[key] = [pos_counts[key]]\n",
    "    return sentence_counts\n",
    "\n",
    "\n",
    "def get_pos_tag_counts(doc):\n",
    "    pos_counts = {}\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    for tag in pos_tags:\n",
    "        if tag in pos_counts:\n",
    "            pos_counts[tag] += 1\n",
    "        else:\n",
    "            pos_counts[tag] = 1\n",
    "    return pos_counts\n",
    "\n",
    "\n",
    "def calculate_mean_per_tag(counts, num_sentences):\n",
    "    mean_pos_tags = {f\"mean_{tag.lower()}\": 0 for tag in POS_TAGS}\n",
    "    for key in counts:\n",
    "        if len(counts[key]) < num_sentences:\n",
    "            counts[key] += [0] * (num_sentences - len(counts[key]))\n",
    "        mean_value = round(np.mean(counts[key]), 2)\n",
    "        mean_pos_tags[\"mean_\" + key.lower()] = mean_value\n",
    "    return mean_pos_tags\n",
    "\n",
    "\n",
    "def get_total_ents(text):\n",
    "    return len(nlp(text).doc.ents)\n",
    "\n",
    "\n",
    "def get_mean_nonstop_char_length_word_count(text):\n",
    "    spans = tokenize_on_stopwords(text)\n",
    "    return sum([get_num_chars(span) for span in spans]) / len(spans),  sum([get_num_words(span) for span in spans]) / len(spans)\n",
    "\n",
    "\n",
    "def get_nonstop_proportion(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    nonstop_tokens = [token for token in tokens if token not in STOPWORDS + PUNCTUATION]\n",
    "    return len(nonstop_tokens) / len(tokens)\n",
    "\n",
    "\n",
    "def tokenize_on_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    spans = []\n",
    "    current_span = []\n",
    "    for token in tokens:\n",
    "        if token not in STOPWORDS + PUNCTUATION:\n",
    "            current_span.append(token)\n",
    "        else:\n",
    "            if len(current_span) > 0:\n",
    "                spans.append(\" \".join(current_span))\n",
    "            current_span = []\n",
    "    return spans\n",
    "\n",
    "\n",
    "def get_num_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def get_num_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def get_num_unique_words(text):\n",
    "     return len(set(w for w in text.split()))\n",
    "\n",
    "\n",
    "def get_num_sentences(text):\n",
    "    total = text.count(\".\") + text.count(\"?\") + text.count(\"!\")\n",
    "    if total == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return total\n",
    "    \n",
    "    \n",
    "def get_num_semicolons(text):\n",
    "    total = text.count(\";\")\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_num_quotes(text):\n",
    "    total = text.count('\"')\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_num_punctuation(text):\n",
    "    total = sum(text.count(w) for w in '.,;:!?\"')\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-aquarium",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:37:00.790370Z",
     "iopub.status.busy": "2021-07-20T10:37:00.789083Z",
     "iopub.status.idle": "2021-07-20T10:37:00.791981Z",
     "shell.execute_reply": "2021-07-20T10:37:00.792456Z",
     "shell.execute_reply.started": "2021-07-18T07:37:39.599093Z"
    },
    "papermill": {
     "duration": 0.037053,
     "end_time": "2021-07-20T10:37:00.792693",
     "exception": false,
     "start_time": "2021-07-20T10:37:00.755640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### FEATURE GENERATION\n",
    "\n",
    "def gen_features(text):\n",
    "    \n",
    "    '''Compute text features'''\n",
    "        \n",
    "    simplified_text = simplify_punctuation(text)\n",
    "\n",
    "    features = {\n",
    "        \"flesch_reading_ease\":          textstat.flesch_reading_ease(simplified_text),\n",
    "        \"smog_index\":                   textstat.smog_index(simplified_text),\n",
    "        \"flesch_kincaid_grade\":         textstat.flesch_kincaid_grade(simplified_text),\n",
    "        \"coleman_liau_index\":           textstat.coleman_liau_index(simplified_text),\n",
    "        \"automated_readability_index\":  textstat.automated_readability_index(simplified_text),\n",
    "        \"dale_chall_readability_score\": textstat.dale_chall_readability_score(simplified_text),\n",
    "        \"difficult_words\":              textstat.difficult_words(simplified_text),\n",
    "        \"linsear_write_formula\":        textstat.linsear_write_formula(simplified_text),\n",
    "        \"gunning_fog2\":                 textstat.gunning_fog(simplified_text),\n",
    "        \"text_standard\":                textstat.text_standard(simplified_text, float_output = True),\n",
    "        \"mean_parse_tree_depth\":        get_mean_parse_tree_depth(text),\n",
    "        \"total_sentences\":              get_num_sentences(text),\n",
    "        \"total_words\":                  get_num_words(text),\n",
    "        \"total_ents\":                   get_total_ents(text),\n",
    "        \"total_chars\":                  get_num_chars(text),\n",
    "        \"total_punctutation\":           get_num_punctuation(text),\n",
    "        \"unique_words\":                 get_num_words(text),\n",
    "        \"nonstop_token_proportion\":     get_nonstop_proportion(text),\n",
    "        \"semicolons\":                   get_num_semicolons(text),\n",
    "        \"quotes\":                       get_num_quotes(text),\n",
    "    }\n",
    "    \n",
    "    nonstops = get_mean_nonstop_char_length_word_count(text)\n",
    "    features['nonstop_char_count'] = nonstops[0]\n",
    "    features['nonstop_word_count'] = nonstops[1]\n",
    "\n",
    "    features['words_per_sentence']    = features['total_words']        / features['total_sentences']\n",
    "    features['ents_per_sentence']     = features['total_ents']         / features['total_sentences']\n",
    "    features['chars_per_sentence']    = features['total_chars']        / features['total_sentences']\n",
    "    features['chars_per_word']        = features['total_chars']        / features['total_words']\n",
    "    features['punctutation_per_word'] = features['total_punctutation'] / features['total_sentences']\n",
    "\n",
    "    features.update(get_mean_pos_tags(text))\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def add_features(data):\n",
    "    \n",
    "    feature_data = []\n",
    "\n",
    "    for text in tqdm(data):\n",
    "        features = gen_features(text)\n",
    "        feature_data.append(features)\n",
    "\n",
    "    return pd.DataFrame(feature_data)\n",
    "\n",
    "\n",
    "def TF_IDF_W2V(text):\n",
    "    '''Calculate TF-IDF with word2vec\n",
    "    '''\n",
    "    #Load TF-IDF from sklearn\n",
    "    TFIDF_model = TfidfVectorizer()\n",
    "    #fit on text\n",
    "    TFIDF_model.fit(text)\n",
    "    #create dictionary with word as key\n",
    "    #and idf as value\n",
    "    dictionary = dict(zip(TFIDF_model.get_feature_names(), list(TFIDF_model.idf_)))\n",
    "    #apply set as we need unique features\n",
    "    TFIDF_words = set(TFIDF_model.get_feature_names())\n",
    "    #create list which stores TFIDF_W2V\n",
    "    TFIDF_W2V_vectors = []\n",
    "    for sentence in text:\n",
    "        #create empty vector to store result\n",
    "        vector = np.zeros(300)\n",
    "        #number of words with valid vector in sentence\n",
    "        TFIDF_weight =0\n",
    "        for word in sentence.split(): \n",
    "            #if word exist in glove_words and TFIDF_words\n",
    "            if (word in glove_words) and (word in TFIDF_words):\n",
    "                #get its vector from glove_words\n",
    "                vec = word2vec_model[word]\n",
    "                #calculate TF-IDF for each word\n",
    "                TFIDF = dictionary[word]*(sentence.count(word)/len(sentence.split()))\n",
    "                #calculate TF-IDF weighted W2V\n",
    "                vector += (vec * TFIDF)\n",
    "                TFIDF_weight += TFIDF\n",
    "                \n",
    "        if TFIDF_weight != 0:\n",
    "            vector /= TFIDF_weight\n",
    "        TFIDF_W2V_vectors.append(vector)\n",
    "    return TFIDF_W2V_vectors \n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "\n",
    "def tf_idf_features(df: pd.DataFrame):\n",
    "\n",
    "    text    = df['excerpt'].apply(lambda x: clean_text(x))\n",
    "    tfidf   = TF_IDF_W2V(text)\n",
    "    vectors = np.array(tfidf)\n",
    "        \n",
    "    return vectors\n",
    "\n",
    "\n",
    "def get_tf_idf_col_names():\n",
    "    names = list()\n",
    "    for i in range(300):\n",
    "        names.append(f\"tf_idf_{i}\")\n",
    "        \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "czech-shaft",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:37:00.818756Z",
     "iopub.status.busy": "2021-07-20T10:37:00.817676Z",
     "iopub.status.idle": "2021-07-20T10:37:00.905107Z",
     "shell.execute_reply": "2021-07-20T10:37:00.904295Z",
     "shell.execute_reply.started": "2021-07-18T07:37:41.466228Z"
    },
    "papermill": {
     "duration": 0.101762,
     "end_time": "2021-07-20T10:37:00.905275",
     "exception": false,
     "start_time": "2021-07-20T10:37:00.803513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### DATA IMPORT\n",
    "\n",
    "train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "packed-bangkok",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:37:00.930309Z",
     "iopub.status.busy": "2021-07-20T10:37:00.929164Z",
     "iopub.status.idle": "2021-07-20T10:55:18.035093Z",
     "shell.execute_reply": "2021-07-20T10:55:18.034278Z",
     "shell.execute_reply.started": "2021-07-18T07:37:48.516979Z"
    },
    "papermill": {
     "duration": 1097.119764,
     "end_time": "2021-07-20T10:55:18.035250",
     "exception": false,
     "start_time": "2021-07-20T10:37:00.915486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [18:17<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "####### COMPUTING FEATURES [1]\n",
    "\n",
    "train_features = add_features(train.excerpt.to_list())\n",
    "train_features = pd.DataFrame(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quarterly-going",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:55:19.814420Z",
     "iopub.status.busy": "2021-07-20T10:55:19.809262Z",
     "iopub.status.idle": "2021-07-20T10:55:27.057884Z",
     "shell.execute_reply": "2021-07-20T10:55:27.057143Z",
     "shell.execute_reply.started": "2021-07-18T08:12:45.75586Z"
    },
    "papermill": {
     "duration": 8.154264,
     "end_time": "2021-07-20T10:55:27.058049",
     "exception": false,
     "start_time": "2021-07-20T10:55:18.903785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### COMPUTING FEATURES [2]\n",
    "\n",
    "train_features2 = pd.DataFrame(train.excerpt.apply(lambda p : readability_measurements(p)).tolist(), \n",
    "                               columns = [\"complex_words\",\"long_words\",\n",
    "                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n",
    "                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n",
    "                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "metric-hours",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T10:55:28.778405Z",
     "iopub.status.busy": "2021-07-20T10:55:28.776924Z",
     "iopub.status.idle": "2021-07-20T10:55:29.139134Z",
     "shell.execute_reply": "2021-07-20T10:55:29.139656Z",
     "shell.execute_reply.started": "2021-07-18T08:13:31.262491Z"
    },
    "papermill": {
     "duration": 1.218666,
     "end_time": "2021-07-20T10:55:29.139843",
     "exception": false,
     "start_time": "2021-07-20T10:55:27.921177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2834, 74)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>...</th>\n",
       "      <th>conjunction</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>preposition</th>\n",
       "      <th>nominalization</th>\n",
       "      <th>pronoun_b</th>\n",
       "      <th>interrogative</th>\n",
       "      <th>article</th>\n",
       "      <th>subordination</th>\n",
       "      <th>conjunction_b</th>\n",
       "      <th>preposition_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>80.11</td>\n",
       "      <td>8.6</td>\n",
       "      <td>6.2</td>\n",
       "      <td>7.65</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>71.24</td>\n",
       "      <td>8.8</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.90</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>75.54</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6.73</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>71.41</td>\n",
       "      <td>6.7</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.08</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>83.12</td>\n",
       "      <td>5.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  \\\n",
       "0  When the young people returned to the ballroom... -0.340259   \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "3  And outside before the palace a great garden w... -1.054013   \n",
       "4  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "   standard_error  flesch_reading_ease  smog_index  flesch_kincaid_grade  \\\n",
       "0        0.464009                80.11         8.6                   6.2   \n",
       "1        0.480805                71.24         8.8                   7.5   \n",
       "2        0.476676                75.54        10.1                   7.9   \n",
       "3        0.450007                71.41         6.7                  11.6   \n",
       "4        0.510845                83.12         5.7                   9.2   \n",
       "\n",
       "   coleman_liau_index  ...  conjunction  pronoun  preposition  nominalization  \\\n",
       "0                7.65  ...           11        8           23               1   \n",
       "1                6.90  ...            7       30           22               0   \n",
       "2                6.73  ...           11       24           18               0   \n",
       "3                8.08  ...           15       12           26               0   \n",
       "4                4.25  ...           10        6           10               0   \n",
       "\n",
       "   pronoun_b  interrogative  article  subordination  conjunction_b  \\\n",
       "0          2              1        2              0              0   \n",
       "1          0              0        0              0              0   \n",
       "2          0              0        0              1              1   \n",
       "3          0              0        0              0              1   \n",
       "4          0              0        0              0              0   \n",
       "\n",
       "   preposition_b  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### SAVE FEATURES\n",
    "\n",
    "df_train = pd.concat([train, train_features, train_features2], axis = 1)\n",
    "df_train.to_csv('features_train.csv', index = False)\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1178.302381,
   "end_time": "2021-07-20T10:55:31.199215",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-20T10:35:52.896834",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
